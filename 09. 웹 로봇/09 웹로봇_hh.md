## 웹 로봇

### 크롤러와 크롤링

- 웹 크롤러는, 먼저 웹 페이지를 한 개 가져오고 그 다음 그 페이지가 가리키는 모든 페이지를 가져오고, 다시 그 페이지들이 그리키는 모든 웹 페이지들을 가져오는 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇.
- 크롤러는 검색한 각 페이지 안에 있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 함.
- 루트 집합(root set) : 크롤러가 방문을 시작하는 URL들의 초기 집합.

### 순환 피하기

- 크롤러를 루프에 빠뜨려서 멈추게 만듦.
- 크롤러가 같은 페이지를 반복해서 가져오면 웹 서버의 부담이 됨.
- 루프 자체가 문제가 되지 않더라도, 크롤러는 많은 수의 중복된 페이지(dups)들을 가져오게 됨.

### 방문한 곳 관리 방법

- 트리 해시 테이블
- 느슨한 존재 비트맵
- 체크포인트
- 파티셔닝

### 루프와 중복 피하기

- URL 정규화
    - URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복된 URL을 일부 회피.
- 너비 우선 크롤링
    - 방문할 URL들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면, 순환의 영향을 최소화할 수 있음.
- 스로틀링
    - 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 스로틀링을 이용해 제한함.
- URL 크기 제한
    - 로봇은 일정 길이(보통 1KB)를 넘는 URL의 크롤링은 거부할 수 있음.
- URL/사이트 블랙리스트
    - 로봇 순환을 만들어 내거나 함정인 것으로 알려진 사이트와 URL 목록을 만들어 관리하고, 그들을 피함.
- 패턴 발견
    - 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있음.
- 콘텐츠 지문(fingerprint)
    - 콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬(checksum)을 계산함.
- 사람의 모니터링
    - 사람이 쉽게 로봇의 진행 상황을 모니터링해서 뭔가 특이한 일이 일어나면 즉각 인지할 수 있게끔 반드시 진단과 로깅을 포함해야 함.

### 요청 헤더 식별하기

- User-Agent : 서버에게 요청을 만든 로봇의 이름.
- From : 로봇의 사용자/관리자의 이메일 주소 제공.
- Accept : 서버에게 어떤 미디어 타입을 보내도 되는지 말해줌.
- Referer : 현재의 요청 URL을 포함한 문서의 URL을 제공.

### 응답 다루기

- 상태 코드 : 로봇들은 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 함.
- 엔터티 : HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있음.

### 웹 사이트와 robots.txt 파일들

- Disallow와 Allow 
    - 로봇 차단 레코드의 User-Agent 줄들 바로 다음에 오며, 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지되어 있고 허용되어 있는지 기술함.
 
- 로봇 META 지시자
    - NOINDEX : 로봇에게 이 페이지를 처리하지 말고 무시하라는 말함.
    - NOFOLLOW : 로봇에게 이 페이지가 링크한 페이지를 크롤링하지 말라고 말함.
    - INDEX : 로봇에게 이 페이지의 콘텐츠를 인덱싱 해도 된다고 말함.
    - FOLLOW : 로봇에게 이 페이지가 링크한 페이지를 크롤링해도 된다고 말함.
    - NOARCHIVE : 로봇에게 이 페이지의 캐시를 위한 로컬 사본을 만들어서 안 된다고말함.
